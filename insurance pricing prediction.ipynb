{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "bb6b542f",
   "metadata": {},
   "source": [
    "# Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f205ba20",
   "metadata": {},
   "outputs": [],
   "source": [
    "# install ProfileReport\n",
    "#!pip install ydata-profiling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a478ba45",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\bethe\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "# import common libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# import specific components from scikit-learn\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score\n",
    "\n",
    "# enhanced stats functions\n",
    "from scipy import stats\n",
    "\n",
    "# for ease of data profiling\n",
    "from ydata_profiling import ProfileReport"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7fc8edbf",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pandas version  : 2.0.2\n",
      "numpy version   : 1.24.3\n",
      "seaborn version : 0.12.2\n"
     ]
    }
   ],
   "source": [
    "# print environment setup details\n",
    "print(f\"pandas version  : {pd.__version__}\")  # 2.0.2\n",
    "print(f\"numpy version   : {np.__version__}\")  # 1.24.3\n",
    "print(f\"seaborn version : {sns.__version__}\") # 0.12.2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b31621c8",
   "metadata": {},
   "source": [
    "Here, we also set the seed for numpy's random number generator such that our results are fully reproducible. This is because the other libraries (e.g. scikit-learn) use this random number generator, so if we set the seed we will always generate the same random numbers in the same sequence.\n",
    "\n",
    "Thus, whenever we run the notebook from top-to-bottom, we will end up with the *exact* same results! "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "15c8144d",
   "metadata": {},
   "outputs": [],
   "source": [
    "SEED = 123\n",
    "np.random.seed(SEED)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a350d6ac",
   "metadata": {},
   "source": [
    "# Data Loading\n",
    "To load in the data for this project, read in `insurance.csv` into a variable called `df` as a pandas DataFrame. The first two rows of the DataFrame should look like this:\n",
    "\n",
    "|       | age | sex     | bmi    | children | smoker | region    | charges     |\n",
    "| ----- | --- | ------- | ------ | -------- | ------ | --------- | ----------- |\n",
    "| **0** | 19  | female  | 27.900 | 0        | yes    | southwest | 16884.92400 |\n",
    "| **1** | 18  | male    | 33.770 | 1        | no     | southeast | 1725.55230  |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "7cef3385",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"C:/Users/bethe/Downloads/insurance.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "c7b56d24",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>age</th>\n",
       "      <th>sex</th>\n",
       "      <th>bmi</th>\n",
       "      <th>children</th>\n",
       "      <th>smoker</th>\n",
       "      <th>region</th>\n",
       "      <th>charges</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>19</td>\n",
       "      <td>female</td>\n",
       "      <td>27.90</td>\n",
       "      <td>0</td>\n",
       "      <td>yes</td>\n",
       "      <td>southwest</td>\n",
       "      <td>16884.9240</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>18</td>\n",
       "      <td>male</td>\n",
       "      <td>33.77</td>\n",
       "      <td>1</td>\n",
       "      <td>no</td>\n",
       "      <td>southeast</td>\n",
       "      <td>1725.5523</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   age     sex    bmi  children smoker     region     charges\n",
       "0   19  female  27.90         0    yes  southwest  16884.9240\n",
       "1   18    male  33.77         1     no  southeast   1725.5523"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# make sure data was read in properly and matches above table\n",
    "df.head(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f036976a",
   "metadata": {},
   "source": [
    "# EDA\n",
    "Now it is time for you to practice your EDA skills! Take a look at the Quest 2 Quiz, and answer the first 5 questions on the results of your EDA on the dataset.\n",
    "\n",
    "**Data dictionary**\n",
    "\n",
    "| column | data definition | \n",
    "| --- | --- | \n",
    "| age | age of insured person | \n",
    "| sex | sex of insured person, either male or female | \n",
    "| bmi | body mass index of insured person |\n",
    "| children | number of dependents covered by health insurance |\n",
    "| smoker | does the insured person smoke? |\n",
    "| region | the insured person's residential area within the US |\n",
    "| charges | medical costs billed by health insurance; target variable |\n",
    "\n",
    "For additional information, check the dataset's [information on Kaggle](https://www.kaggle.com/datasets/mirichoi0218/insurance)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "db665e03",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Summarize dataset: 100%|██████████| 32/32 [00:05<00:00,  5.69it/s, Completed]                 \n",
      "Generate report structure: 100%|██████████| 1/1 [00:03<00:00,  3.58s/it]\n",
      "Render HTML: 100%|██████████| 1/1 [00:02<00:00,  2.24s/it]\n",
      "Export report to file: 100%|██████████| 1/1 [00:00<00:00, 112.25it/s]\n"
     ]
    }
   ],
   "source": [
    "# optional: use pandas-profiling to do quick first pass at EDA\n",
    "profile = ProfileReport(df, title=\"Insurance Dataset Profiling Report\")\n",
    "# create html file to view report\n",
    "profile.to_file(\"insurance_dataset_report.html\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73741f16",
   "metadata": {},
   "outputs": [],
   "source": [
    "# conduct any other EDA that you need to in order to get a good feel for the data\n",
    "df.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70f9aaa8",
   "metadata": {},
   "source": [
    "With the EDA you have conducted, answer the following questions from the quiz. Note that these questions do not cover everything you should be looking for when doing EDA, they are just to give you an idea of what EDA would look like on such a dataset.\n",
    "## Question 1\n",
    "What is the median age of the individuals in our dataset?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e62c0a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# calculate median age\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c44b7ca6",
   "metadata": {},
   "source": [
    "## Question 2\n",
    "What is the index of the person who has the highest BMI?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09365f4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# find index of person with highest BMI\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bccf2b98",
   "metadata": {},
   "source": [
    "## Question 3\n",
    "Which two independent numeric variables (excluding categorical and dummy variables) are most highly correlated with each other?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ef19dd7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot correlation heatmap\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a1788b6",
   "metadata": {},
   "source": [
    "## Question 4\n",
    "What is the distribution of the `charges` variable in our dataset?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92d8e2b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot distribution of charges\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f484be7a",
   "metadata": {},
   "source": [
    "That's strange, that doesn't look like our data is normally distributed. While you can theoretically still apply linear regression to a distribution which isn't normally distributed, it's generally best practice for your data to be normal. Looks like we may need to transform our data in some way in order to get it closer to a normal distribution.\n",
    "\n",
    "Create a new column in df, `log_charges`, by taking the log of the existing `charges` column. Then, drop the `charges` column. Your first two rows should now look like:\n",
    "\n",
    "|       | age | sex     | bmi    | children | smoker | region    | log_charges |\n",
    "| ----- | --- | ------- | ------ | -------- | ------ | --------- | ----------- |\n",
    "| **0** | 19  | female  | 27.900 | 0        | yes    | southwest | 9.734176    |\n",
    "| **1** | 18  | male    | 33.770 | 1        | no     | southeast | 7.453302    |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d00b7646",
   "metadata": {},
   "outputs": [],
   "source": [
    "# take log of charges, then drop old column\n",
    "df[\"log_charges\"] = "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4502e547",
   "metadata": {},
   "outputs": [],
   "source": [
    "# check if your table matches the one above\n",
    "df.head(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a879ebe3",
   "metadata": {},
   "source": [
    "Great, now plot the new distribution to see whether it's a normal distribution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9b78c19",
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot distribution for log_charges"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b521aec7",
   "metadata": {},
   "source": [
    "Note that even after log transforming our data, it is still not normally distributed. But it is now closer to a normal distribution than the data originally was, so we can continue on with our analysis using this new variable `log_charges` variable as our target variable instead!\n",
    "\n",
    "### Optional exploration\n",
    "Are there other ways of determining whether the data is normally distributed or not, besides looking at the above plot? Hint: they're in the stats package we already imported from scipy!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98afb223",
   "metadata": {},
   "source": [
    "## Question 5\n",
    "There are two rows which are identical to each other; what are their indexes?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca56da3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# find indexes of duplicated rows"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8c3de18",
   "metadata": {},
   "source": [
    "# Data Wrangling\n",
    "Now that we have an idea of what our data looks like, we need to start preparing it for modelling.\n",
    "\n",
    "In this case, we don't have any missing data, so we don't need to do anything about that. We have one duplicated row, but that seems to be legitimate data, so we will keep it in. \n",
    "\n",
    "We have 3 categorical variables - sex, smoker, and region. Get dummy variables using pandas, ensuring to use the `drop_first=True` argument to mitigate possible multicollinearity issues. As a result, you should only have one dummy variable for binary values such as `sex` or `smoker`. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a889781a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# get dummies for sex, smoker, and region\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bea93543",
   "metadata": {},
   "source": [
    "Now verify that we still don't have major multicollinearity issues with a heatmap. What we're looking for is that most of the correlations between independent variables should still be relatively low. There's no single cut-off value (much of this is as much an art as it is a science), but we'll say for our purposes here that we'll consider anything between -0.5 and 0.5 to be low.\n",
    "\n",
    "Note that if a variable is strongly correlated with our dependent variable `log_charges`, that's okay. If anything, that's probably a good thing!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cbd876f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# correlation plot heatmap\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a4a7d88",
   "metadata": {},
   "source": [
    "## Question 6\n",
    "After getting dummy variables from our categorical variables, how many independent variables do we have?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f491e152",
   "metadata": {},
   "outputs": [],
   "source": [
    "# get number of independent variables\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e16fadca",
   "metadata": {},
   "source": [
    "Now separate our independent variables into a variable called `X`, and our target variable `log_charges` into a variable called `y`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9032f0ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "# split up our variables\n",
    "X = \n",
    "y = "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "208e0d3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# the resulting shapes of X and y should be (1338, 8) and (1338,) respectively\n",
    "print(X.shape)\n",
    "print(y.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e27a566",
   "metadata": {},
   "source": [
    "Now we need to split up our data into training and test data. Using scikit-learn's `train_test_split` function, using a `test_size` of 0.3 (i.e. 30% of data in test set), and **ensure that the random state is set to our seed from above**.\n",
    "\n",
    "Documentation for `train_test_split()` can be found here: https://scikit-learn.org/1.1/modules/generated/sklearn.model_selection.train_test_split.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12bbb2e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# do the train test split\n",
    "X_train, X_test, y_train, y_test = "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "699f120b",
   "metadata": {},
   "source": [
    "### Question 7\n",
    "What are the indexes for the first five data points in the training dataset after train_test_split?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31311d16",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "f6471b31",
   "metadata": {},
   "source": [
    "# Modelling and Evaluation\n",
    "Now we can make our model! Instantiate a LinearRegression model in scikit-learn, then fit the training data on it.\n",
    "\n",
    "Documentation for `LinearRegression()` can be found here: https://scikit-learn.org/1.1/modules/generated/sklearn.linear_model.LinearRegression.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4bd8bbb6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# instantiate linear regression model and fit the training data to it\n",
    "lr = "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c1aed74",
   "metadata": {},
   "outputs": [],
   "source": [
    "# describe model\n",
    "print(f\"intercept: {lr.intercept_:.4f}\")\n",
    "for i in range(len(X_train.columns)):\n",
    "    print(f\"{X_train.columns[i]}: {lr.coef_[i]:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f5b3c51",
   "metadata": {},
   "source": [
    "Now we need to evaluate how our model did. Let's start by gathering some basic metrics on the training data (e.g. MAE, RMSE, R^2), then do the same on the test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0890be25",
   "metadata": {},
   "outputs": [],
   "source": [
    "# predict training set values\n",
    "predictions_train = "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "baa11773",
   "metadata": {},
   "outputs": [],
   "source": [
    "# evaluate model R^2 (score)\n",
    "r2_train = "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7c183b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# evaluate model MAE\n",
    "mae_train = "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "490012e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# evaluate model RMSE\n",
    "rmse_train = "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a9a4606",
   "metadata": {},
   "outputs": [],
   "source": [
    "# print metrics\n",
    "print(\"metrics for training data\")\n",
    "print(f\"R^2 score : {r2_train:.4f}\")\n",
    "print(f\"mae       : {mae_train:.4f}\")\n",
    "print(f\"rmse      : {rmse_train:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0ec8da3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot predictions against actual values\n",
    "plt.scatter(y_train, predictions_train)\n",
    "plt.title(\"training dataset\")\n",
    "plt.xlabel(\"actual\")\n",
    "plt.ylabel(\"predicted\")\n",
    "plt.gca().axline([7, 7], [11, 11], color=\"red\")\n",
    "plt.grid()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8be17be4",
   "metadata": {},
   "source": [
    "Clearly, while our model has clearly learned something about the data, there is still room for improvement.\n",
    "\n",
    "Now we're going to evaluate our model's performance on the test set. First, we need to make our model predict the values for the test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79fe112f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# make predictions on test set\n",
    "predictions_test = "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e89d084d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# get same metrics as before - R^2, MAE, RMSE\n",
    "r2_test = \n",
    "mae_test = \n",
    "rmse_test = "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e14094c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# print metrics\n",
    "print(\"metrics for test data\")\n",
    "print(f\"R^2 score : {r2_test:.4f}\")\n",
    "print(f\"mae       : {mae_test:.4f}\")\n",
    "print(f\"rmse      : {rmse_test:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "375ff832",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# plot predictions against actual values\n",
    "plt.scatter(y_test, predictions_test)\n",
    "plt.title(\"test dataset\")\n",
    "plt.xlabel(\"actual\")\n",
    "plt.ylabel(\"predicted\")\n",
    "plt.gca().axline([7, 7], [11, 11], color=\"red\")\n",
    "plt.grid()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ee37313",
   "metadata": {},
   "source": [
    "## Question 8\n",
    "Are the residuals normally distributed?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a308e26",
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot residuals to determine if they are normally distributed\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b28e638b",
   "metadata": {},
   "source": [
    "Now, let's take a step back. We know that we took the log of `charges` in order to make it closer to a normal distribution. While this is helpful for our modelling, at the end of the day we want to predict the actual insurance charges, not the log of them.\n",
    "\n",
    "## Question 9\n",
    "What are the predicted insurance charges for index 650 (the first row in the test dataset), rounded to 2 decimal places (i.e. nearest cent)?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "374bbf89",
   "metadata": {},
   "outputs": [],
   "source": [
    "# get predicted insurance charge for index 650\n",
    "charge = "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87c441d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# get the predicted insurance charges\n",
    "print(f\"predicted insurance charges: {charge:.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3950fbf",
   "metadata": {},
   "source": [
    "# Discussion\n",
    "While our model has learned some of the relationships from the data, it is still imperfect. What else could we do to improve it? Here are some possibilities:\n",
    "1. We did not normalize/standardize our data, which could result in larger numeric variables (e.g. age) having a larger impact on the final result than smaller numeric variables (e.g. children). For more on normalization and standardization, check out [this resource](https://www.analyticsvidhya.com/blog/2020/04/feature-scaling-machine-learning-normalization-standardization/).\n",
    "2. As our region dummy variables seem somewhat negatively correlated with each other, perhaps we may have issues with multicollinearity. So we could try encoding the region variable differently, separating into just 2 variables, one for north/south and the other for east/west.\n",
    "3. Perhaps some of the variables interact with each other. We could try adding additional features covering such interactions (e.g. smoking & sex, region & bmi, etc.). Check out [this resource for more information on interaction effects](https://stattrek.com/multiple-regression/interaction).\n",
    "4. Instead of normalizing our `charges` value by taking the log of it, we could do a Box-Cox transformation instead. Check out [this resource for more on Box-Cox transformation](https://towardsdatascience.com/box-cox-transformation-explained-51d745e34203).\n",
    "\n",
    "Implementing any of the suggested potential improvements can be a task for you to work on"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
